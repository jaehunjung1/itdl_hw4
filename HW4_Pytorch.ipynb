{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019 Introduction to Deep Learning HW4: Fake News Generator!\n",
    "\n",
    "Created by Yeon-goon Kim, SNU ECE, CML.\n",
    "\n",
    "On this homework, you will create fake news generator, which is basic RNN/LSTM/GRU char2char generate model. Of course, your results may not so good, but you can expect some sentence-like results by doing this homework sucessfully.\n",
    "\n",
    "## Now, We'll handle texts, not images. Is there any differences?\n",
    "\n",
    "Of course, there are many differences between processing images and texts. One is that text cannot be expressed directly to matrix or tensor. We know an image can be expressed as Tensor(n_channel, width, height). But how about text? Can word 'Homework' can be expressed to tensor directly? By what laws? With what shapes? Even if it can, which one is closer to that word, 'Burden', or 'Work'? This is called 'Word Embedding Problem' and be considered as one of the most important problem in Natural Language Process(NLP) resarch. Fortunatly, there are some generalized solution in this problem (though not prefect, anyway) and both Tensorflow(Keras) and Pytorch give basic API that automatically solve this problem. You may investigate and use those APIs in this homework. \n",
    "\n",
    "The other one is that text is sequential data. Generally when processing images, without batch, input is just one image. However in text, input is mostly some or one paragraphs/sentences, which is sequential data of embedded characters or words. So, If we want to generate word 'Homework' with start token 'H', 'o' before 'H' and 'o' before 'Homew' should operate different when it gives to input. This is why we use RNN-based model in deep learning when processing text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement\n",
    "In this homework I recommend that you should use the latest version of Pytorch, which is on now(2019-11-19) Pytorch 1.3.x.. Maybe you should use python3.7 because python3.8 may not compatible and inconsistent now. And to use dataset, you must install 'pandas' package, which that give convinience to read and manipulate .csv files. You can easilly install the package with command 'pip install pandas' or with conda if you use conda venv. Don't be so worry that you don't need to know how to use it, data pre-process code will be given. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages & Create Dataset\n",
    "These codes will create dataset that automatically change each character in texts to int, which is assigned index by vocab.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### This Code should not be changed except 'USE_GPU'. Please mail to T/A if you must need to change with proper description.\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import math\n",
    "\n",
    "########### Change whether you would use GPU on this homework or not ############\n",
    "USE_GPU = False\n",
    "#################################################################################\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "vocab = open('vocab.txt').read().splitlines()\n",
    "n_vocab = len(vocab)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Change char to index.\n",
    "def text2int(csv_file, dname, vocab):\n",
    "    ret = []\n",
    "    data = csv_file[dname].values\n",
    "    for datum in data:\n",
    "        for char in str(datum):\n",
    "            idx = vocab.index(char)\n",
    "            ret.append(idx)    \n",
    "    ret = np.array(ret)\n",
    "    return ret\n",
    "\n",
    "# Create dataset to automatically iterate.\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, csv_file, vocab):\n",
    "        self.csv_file = pd.read_csv(csv_file, sep='|')\n",
    "        self.vocab = vocab\n",
    "        # self.len = len(self.csv_file)\n",
    "\n",
    "        self.len = 2645\n",
    "        self.x_data = torch.tensor(text2int(self.csv_file, 'x_data', self.vocab)[:169280]).view(-1, 64)\n",
    "        self.y_data = torch.tensor(text2int(self.csv_file, 'y_data', self.vocab)[:169280]).view(-1, 64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1: RNN/LSTM/GRU Module\n",
    "\n",
    "The main task is to create RNN/LSTM/GRU network that both input & output shape is (batch_size, vocab_size). You can use Pytorch api such as nn.XXX or barebone torch with F. I recommend use nn.XXX and module form that described on under, but you can use any of pytorch api that basically given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### WRITE DOWN YOUR CODE ################################\n",
    "## Task_recommended form. You can use another form such as nn.Sequential or barebone Pytorch if you want, but in that case you may need to change some test or train code that given on later.\n",
    "class CharacterLSTM(nn.Module):\n",
    "    def __init__(self, vocab_len, device):\n",
    "        super(CharacterLSTM, self).__init__()\n",
    "        self.vocab_len = vocab_len\n",
    "        self.device = device\n",
    "        self.lstm = nn.LSTM(self.vocab_len, hidden_size=256,\n",
    "                            batch_first=True,\n",
    "                            dropout=0.3, num_layers=2)\n",
    "        self.linear = nn.Linear(256, vocab_len)\n",
    "\n",
    "    def forward(self, x):  # x : (batch_size, sentence_len)\n",
    "        out = torch.arange(self.vocab_len).view(1, 1, -1).repeat((*x.size(), 1)).to(self.device)\n",
    "        out = (out == x.unsqueeze(2)).float()\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.linear(out)\n",
    "        return out  # (64, 1, vocab_len)\n",
    "#################### WRITE DOWN YOUR CODE ################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Task: Train Code\n",
    "\n",
    "This code would define train function that train network that defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "###################### Train Code. On mostly you don't need to change this, but ok if you really need to do it.\n",
    "def train(dataset, model, optimizer, n_iters):\n",
    "    model.to(device=device)\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    print_every = 50\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for e in range(n_iters):\n",
    "        for i, (x, y) in enumerate(dataset):\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            model.zero_grad()\n",
    "            output = model(x)  # output: (batch_size, sentence_len, vocab_len)\n",
    "            loss = criterion(output.view(-1, vocab_len), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if e % print_every == 0:\n",
    "            print(f\"Iteration {e}, {e / n_iters * 100} | {time_since(start)}, Loss: {loss}\")\n",
    "            torch.save(model.state_dict(), f'./fng_pt.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Task: Test Code\n",
    "\n",
    "This code would define test function that test network by generating (max_length)-length character sequence from 'start_letter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Test Code. On mostly you don't need to change this except value of 'max_length', but ok if you really need to do it.\n",
    "def test(start_letter):\n",
    "    max_length = 1000\n",
    "    with torch.no_grad():\n",
    "        idx = vocab.index(start_letter)\n",
    "        input_nparray = [idx]\n",
    "        input_nparray = np.reshape(input_nparray, (1, len(input_nparray)))\n",
    "        inputs = torch.tensor(input_nparray, device=device, dtype=torch.long)\n",
    "        output_sen = start_letter\n",
    "        for i in range(max_length):\n",
    "            output = model(inputs).squeeze(0)\n",
    "            # topv, topi = output.topk(5)\n",
    "            # topi = topi[-1][torch.multinomial(topv[-1], 1)]  # sample from top 5\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[-1]\n",
    "            letter = vocab[topi]\n",
    "            output_sen += letter\n",
    "            idx = vocab.index(letter)\n",
    "            input_nparray = np.append(input_nparray, [idx])\n",
    "            inputs = torch.tensor(input_nparray, device=device, dtype=torch.long).unsqueeze(0)\n",
    "    return output_sen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2: Train & Generate\n",
    "\n",
    "Using above defined functions and network, Do your train process and show your results freely! Since this is generating tasks so there are no clear test set, credits are given based on quality of generated sequence. Please see the document to find criterion. (Hint: See your loss carefully, and if final loss is between 1~2 or more, you will get results that match to basic credit. If final loss is under ~0.1, you will get results that match to full credit.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n",
      "White and the Seven Dwarfs Moria wrete of Mr. Trump’s basial bower sayia cornull to their sideas and dosser to the endired the plants on the system to preventing them after decades of fabric posters, they was the friends and barnest stape father. He said. “There was still surpers in the city. “I was going to take my two painting both straved it was also leader as the shipping scients as a landscape prints of dolbars of the book what he had done it is Houst on Starios Coust A time than $500 and a dozed by made more things — graduations on the political narrative of the country’s most gunnsy. The fabric resord that experien in the Bronx, much of the plant — was because of a specific community,” he said. Groups like the services are artist can plant posty growing 40 millions and permanity,” said He said Dude test to put the city’s los — with his summer, with time, Ms. Kerr said, “I called my friends in the city. In a locais from about $15, that showed by Mr. Trump calls nemelt, even as it \n"
     ]
    }
   ],
   "source": [
    "print('using device:', device)\n",
    "vocab_len = len(vocab)\n",
    "model = CharacterLSTM(vocab_len, device)\n",
    "\n",
    "do_restore = True\n",
    "\n",
    "if do_restore:\n",
    "    model.load_state_dict(torch.load('fng_pt.pt', map_location=lambda storage, location: storage))\n",
    "    model.eval()\n",
    "    model.to(device=device)\n",
    "else:\n",
    "    n_iters = 500\n",
    "    dataset = NewsDataset(csv_file='data.csv', vocab=vocab)\n",
    "    train_loader = DataLoader(dataset=dataset, batch_size=64, shuffle=False, num_workers=1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=2e-16, weight_decay=0)\n",
    "    train(train_loader, model, optimizer, n_iters)\n",
    "\n",
    "print(test('W'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
